{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Тюнинг\n",
    "import optuna as opt\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.model_selection import cross_val_score, train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "# Пайплайн\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import TransformerMixin, BaseEstimator\n",
    "\n",
    "# Данные\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from category_encoders import BinaryEncoder, OneHotEncoder\n",
    "from sklearn.preprocessing import RobustScaler, StandardScaler, MinMaxScaler, MaxAbsScaler\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Пути\n",
    "ROOT = os.getcwd()\n",
    "TRAIN_DATASET = os.path.join(ROOT, '../data/train_AIC.csv')\n",
    "BALANCED_DATASET = os.path.join(ROOT, '../data/balanced_train.csv')\n",
    "TEST_DATASET = os.path.join(ROOT, '../data/test_AIC.csv')\n",
    "SUBMISSION_PATH = os.path.join(ROOT, '../submissions/')\n",
    "\n",
    "def save_submission(model, subname):\n",
    "    subname = os.path.join(SUBMISSION_PATH, f'{subname}.csv')\n",
    "    preds = model.predict(test_df)\n",
    "    submit_df = pd.DataFrame({'id': test_df.index, 'value': preds})\n",
    "    submit_df.to_csv(subname, index=False)\n",
    "\n",
    "# Загрузка\n",
    "train_df = pd.read_csv(TRAIN_DATASET)\n",
    "# balanced_df = pd.read_csv(BALANCED_DATASET, index_col=0)\n",
    "test_df = pd.read_csv(TEST_DATASET)\n",
    "\n",
    "first_negatives = train_df[train_df['y'] == 0][:train_df[train_df['y'] == 1]['y'].count()]\n",
    "train_df = pd.concat([train_df[train_df['y'] == 1], first_negatives])\n",
    "\n",
    "def random_undersample(df):\n",
    "    neg_count, pos_count = np.bincount(df['y'])\n",
    "    pos_df = df[df['y'] == 1]\n",
    "    neg_df = df[df['y'] == 0]\n",
    "    neg_df = neg_df.sample(n=pos_count, random_state=1708)\n",
    "    return pd.concat([pos_df, neg_df])\n",
    "\n",
    "balanced_df = random_undersample(train_df)\n",
    "\n",
    "X, y = balanced_df.iloc[:, :-1], balanced_df.iloc[:, -1]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_features = [\n",
    "    'Provider'\n",
    "    ]\n",
    "\n",
    "scale_features = [\n",
    "    'Sum', 'Position Count', 'Duration', 'ETC Delivery',\n",
    "    'Changes After Approvals', 'Order Approval 1', 'Order Approval 2',\n",
    "    'Order Approval 3'\n",
    "]\n",
    "\n",
    "drop_features = [\n",
    "    'Change on Paper', 'Amount', 'Category Manager', 'Factory', \n",
    "    'Material', 'Cancel Complete Release', 'Approval Cycles',\n",
    "    'Change Delivery Date 15', 'Change Delivery Date 30', \n",
    "    'Company Code', 'EI', 'Delivery Option', 'Days Between 4_5',\n",
    "    'Days Between 5_6', 'Days Between 6_7', 'Purchasing Organization',\n",
    "    'Purchasing Group', 'Material Group', 'Days Between 1_2', \n",
    "    'Days Between 2_3', 'Days Between 3_4', 'Days Between 7_8',\n",
    "    'Delivery Date', 'Days Between 0_1', 'NRP', 'Operations Manager'\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Препроцессоры\n",
    "class DataPreprocessor(BaseEstimator, TransformerMixin):\n",
    "    \"\"\" Предобработчик данных \"\"\"\n",
    "    def __init__(self, cat_features, scale_features,\n",
    "                 drop_features, transform_train=True):\n",
    "        self.transform_train = transform_train\n",
    "        self.cat_features = cat_features\n",
    "\n",
    "        self.bin_encoder = BinaryEncoder(cols=cat_features)\n",
    "        self.onehot_encoder = OneHotEncoder(cols=['Поставщик'])\n",
    "        self.robust_scaler = RobustScaler()\n",
    "        self.rename_cols = (\n",
    "            'Provider', 'Material', 'Category Manager', 'Operations Manager',\n",
    "            'Factory', 'Purchasing Organization', 'Purchasing Group', \n",
    "            'Company Code', 'EI', 'Material Group', 'Delivery Option', 'NRP',\n",
    "            'Duration', 'ETC Delivery', 'Month1', 'Month2', 'Month3', 'Weekday',\n",
    "            'Sum', 'Position Count', 'Amount', 'Handlers 7', 'Handlers 15', \n",
    "            'Handlers 30', 'Order Approval 1', 'Order Approval 2', 'Order Approval 3',\n",
    "            'Change Delivery Date 7', 'Change Delivery Date 15', 'Change Delivery Date 30',\n",
    "            'Cancel Complete Release', 'Change on Paper', 'Delivery Date', \n",
    "            'Approval Cycles', 'Changes After Approvals', 'Days Between 0_1', \n",
    "            'Days Between 1_2', 'Days Between 2_3', 'Days Between 3_4', 'Days Between 4_5', \n",
    "            'Days Between 5_6', 'Days Between 6_7', 'Days Between 7_8'\n",
    "            )\n",
    "\n",
    "        self.drop_features = drop_features\n",
    "        self.scale_features = scale_features\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        # Создаём копию датасета\n",
    "        X_ = X.copy()\n",
    "        X_.columns = self.rename_cols\n",
    "        X_ = X_.drop(self.drop_features, axis=1)\n",
    "\n",
    "        # Временные фичи\n",
    "        X_['Weekday'] += 1\n",
    "        X_['day_sin'] = np.sin(np.pi * 2 * X_['Weekday'] / 7)\n",
    "        X_['day_cos'] = np.cos(np.pi * 2 * X_['Weekday'] / 7)\n",
    "        X_['month1_sin'] = np.sin(np.pi * 2 * X_['Month1'] / 12)\n",
    "        X_['month1_cos'] = np.cos(np.pi * 2 * X_['Month1'] / 12)\n",
    "        X_['month2_sin'] = np.sin(np.pi * 2 * X_['Month2'] / 12)\n",
    "        X_['month2_cos'] = np.cos(np.pi * 2 * X_['Month2'] / 12)\n",
    "        X_['month3_sin'] = np.sin(np.pi * 2 * X_['Month3'] / 12)\n",
    "        X_['month3_cos'] = np.cos(np.pi * 2 * X_['Month3'] / 12)\n",
    "\n",
    "        # Категориальные фичи\n",
    "        X_ = self.bin_encoder.fit_transform(X_)\n",
    "\n",
    "        features_to_drop = ['Weekday', 'Month1', 'Month2', 'Month3']\n",
    "        X_ = X_.drop(features_to_drop, axis=1)\n",
    "\n",
    "        # Масштабирование\n",
    "        self.robust_scaler.fit(X_[self.scale_features])\n",
    "\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        # Создаём копию датасета\n",
    "        X_ = X.copy()\n",
    "        X_.columns = self.rename_cols\n",
    "        X_ = X_.drop(self.drop_features, axis=1)\n",
    "\n",
    "        # Временные фичи\n",
    "        X_['day_sin'] = np.sin(np.pi * 2 * X_['Weekday'] / 7)\n",
    "        X_['day_cos'] = np.cos(np.pi * 2 * X_['Weekday'] / 7)\n",
    "        X_['month1_sin'] = np.sin(np.pi * 2 * X_['Month1'] / 12)\n",
    "        X_['month1_cos'] = np.cos(np.pi * 2 * X_['Month1'] / 12)\n",
    "        X_['month2_sin'] = np.sin(np.pi * 2 * X_['Month2'] / 12)\n",
    "        X_['month2_cos'] = np.cos(np.pi * 2 * X_['Month2'] / 12)\n",
    "        X_['month3_sin'] = np.sin(np.pi * 2 * X_['Month3'] / 12)\n",
    "        X_['month3_cos'] = np.cos(np.pi * 2 * X_['Month3'] / 12)\n",
    "\n",
    "        # Категориальные фичи\n",
    "        X_ = self.bin_encoder.transform(X_)\n",
    "\n",
    "        features_to_drop = ['Weekday', 'Month1', 'Month2', 'Month3']\n",
    "\n",
    "        X_ = X_.drop(features_to_drop, axis=1)\n",
    "\n",
    "        # Масштабирование\n",
    "        X_[self.scale_features] = self.robust_scaler.transform(X_[self.scale_features])\n",
    "\n",
    "        return X_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_preprocessor.fit(X_train)\n",
    "# X_train_preproc = data_preprocessor.transform(X_train)\n",
    "# X_test_preproc = data_preprocessor.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция оптимизации\n",
    "def objective(trial: opt.Trial):\n",
    "    # Параметры\n",
    "    learning_rate = trial.suggest_float('learning_rate', 0.1, 1, log=True)\n",
    "    n_estimators = trial.suggest_int('n_estimators', 700, 1500, 50)\n",
    "    max_depth = trial.suggest_int('max_depth', 6, 16)\n",
    "    max_bin = trial.suggest_int('max_bin', 64, 352),\n",
    "    num_leaves = trial.suggest_int('num_leaves', 64, 256)\n",
    "    reg_lambda = trial.suggest_float('l2_reg', 0.1, 1, log=True)\n",
    "\n",
    "    # Модель\n",
    "    data_preprocessor = DataPreprocessor(cat_features)\n",
    "    model = LGBMClassifier(\n",
    "        learning_rate=learning_rate,\n",
    "        n_estimators=n_estimators,\n",
    "        max_depth=max_depth,\n",
    "        reg_lambda=reg_lambda,\n",
    "        max_bin=max_bin,\n",
    "        n_jobs=-1,\n",
    "        force_col_wise=True\n",
    "    )\n",
    "\n",
    "    rfe = RFE(estimator=model)\n",
    "\n",
    "    pipeline = Pipeline([\n",
    "        ('data_preproc', data_preprocessor),\n",
    "        ('model', model)\n",
    "    ])\n",
    "    \n",
    "    cv_score = cross_val_score(pipeline, X_train, y_train, cv=StratifiedKFold(n_splits=5), scoring='f1_macro', n_jobs=-1)\n",
    "    accuracy = cv_score.mean()\n",
    "\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study = opt.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = {\n",
    "    'learning_rate': 0.3, \n",
    "    'n_estimators': 1000, \n",
    "    'max_depth': 12, \n",
    "    'max_bin': 128, \n",
    "    'num_leaves': 128, \n",
    "    'reg_lambda': 0.2,\n",
    "    }\n",
    "\n",
    "# Модель\n",
    "data_preprocessor = DataPreprocessor(cat_features, scale_features, drop_features)\n",
    "# model = LGBMClassifier(\n",
    "#     **best_params,\n",
    "#     n_jobs=-1,\n",
    "#     force_col_wise=True,\n",
    "#     is_unbalance=True\n",
    "# )\n",
    "\n",
    "model = GaussianProcessClassifier()\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('data_preproc', data_preprocessor),\n",
    "    ('model', model)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 12.6 GiB for an array with shape (1693940115,) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mpipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m f1_score(y_test, pipeline\u001b[38;5;241m.\u001b[39mpredict(X_test), average\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmacro\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mo:\\Programing\\predict-supply-disruption\\env\\Lib\\site-packages\\sklearn\\base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1144\u001b[0m     estimator\u001b[39m.\u001b[39m_validate_params()\n\u001b[0;32m   1146\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\n\u001b[0;32m   1147\u001b[0m     skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[0;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1149\u001b[0m     )\n\u001b[0;32m   1150\u001b[0m ):\n\u001b[1;32m-> 1151\u001b[0m     \u001b[39mreturn\u001b[39;00m fit_method(estimator, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mo:\\Programing\\predict-supply-disruption\\env\\Lib\\site-packages\\sklearn\\pipeline.py:420\u001b[0m, in \u001b[0;36mPipeline.fit\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    418\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_final_estimator \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mpassthrough\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    419\u001b[0m         fit_params_last_step \u001b[39m=\u001b[39m fit_params_steps[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msteps[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m][\u001b[39m0\u001b[39m]]\n\u001b[1;32m--> 420\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_final_estimator\u001b[39m.\u001b[39;49mfit(Xt, y, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_params_last_step)\n\u001b[0;32m    422\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[1;32mo:\\Programing\\predict-supply-disruption\\env\\Lib\\site-packages\\sklearn\\base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1144\u001b[0m     estimator\u001b[39m.\u001b[39m_validate_params()\n\u001b[0;32m   1146\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\n\u001b[0;32m   1147\u001b[0m     skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[0;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1149\u001b[0m     )\n\u001b[0;32m   1150\u001b[0m ):\n\u001b[1;32m-> 1151\u001b[0m     \u001b[39mreturn\u001b[39;00m fit_method(estimator, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mo:\\Programing\\predict-supply-disruption\\env\\Lib\\site-packages\\sklearn\\gaussian_process\\_gpc.py:741\u001b[0m, in \u001b[0;36mGaussianProcessClassifier.fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    738\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    739\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mUnknown multi-class mode \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmulti_class)\n\u001b[1;32m--> 741\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbase_estimator_\u001b[39m.\u001b[39;49mfit(X, y)\n\u001b[0;32m    743\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_classes_ \u001b[39m>\u001b[39m \u001b[39m2\u001b[39m:\n\u001b[0;32m    744\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlog_marginal_likelihood_value_ \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mmean(\n\u001b[0;32m    745\u001b[0m         [\n\u001b[0;32m    746\u001b[0m             estimator\u001b[39m.\u001b[39mlog_marginal_likelihood()\n\u001b[0;32m    747\u001b[0m             \u001b[39mfor\u001b[39;00m estimator \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbase_estimator_\u001b[39m.\u001b[39mestimators_\n\u001b[0;32m    748\u001b[0m         ]\n\u001b[0;32m    749\u001b[0m     )\n",
      "File \u001b[1;32mo:\\Programing\\predict-supply-disruption\\env\\Lib\\site-packages\\sklearn\\gaussian_process\\_gpc.py:256\u001b[0m, in \u001b[0;36m_BinaryGaussianProcessClassifierLaplace.fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    254\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlog_marginal_likelihood_value_ \u001b[39m=\u001b[39m \u001b[39m-\u001b[39mnp\u001b[39m.\u001b[39mmin(lml_values)\n\u001b[0;32m    255\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 256\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlog_marginal_likelihood_value_ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlog_marginal_likelihood(\n\u001b[0;32m    257\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mkernel_\u001b[39m.\u001b[39;49mtheta\n\u001b[0;32m    258\u001b[0m     )\n\u001b[0;32m    260\u001b[0m \u001b[39m# Precompute quantities required for predictions which are independent\u001b[39;00m\n\u001b[0;32m    261\u001b[0m \u001b[39m# of actual query points\u001b[39;00m\n\u001b[0;32m    262\u001b[0m K \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkernel_(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mX_train_)\n",
      "File \u001b[1;32mo:\\Programing\\predict-supply-disruption\\env\\Lib\\site-packages\\sklearn\\gaussian_process\\_gpc.py:381\u001b[0m, in \u001b[0;36m_BinaryGaussianProcessClassifierLaplace.log_marginal_likelihood\u001b[1;34m(self, theta, eval_gradient, clone_kernel)\u001b[0m\n\u001b[0;32m    379\u001b[0m     K, K_gradient \u001b[39m=\u001b[39m kernel(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mX_train_, eval_gradient\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m    380\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 381\u001b[0m     K \u001b[39m=\u001b[39m kernel(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mX_train_)\n\u001b[0;32m    383\u001b[0m \u001b[39m# Compute log-marginal-likelihood Z and also store some temporaries\u001b[39;00m\n\u001b[0;32m    384\u001b[0m \u001b[39m# which can be reused for computing Z's gradient\u001b[39;00m\n\u001b[0;32m    385\u001b[0m Z, (pi, W_sr, L, b, a) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_posterior_mode(K, return_temporaries\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mo:\\Programing\\predict-supply-disruption\\env\\Lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:945\u001b[0m, in \u001b[0;36mProduct.__call__\u001b[1;34m(self, X, Y, eval_gradient)\u001b[0m\n\u001b[0;32m    941\u001b[0m     \u001b[39mreturn\u001b[39;00m K1 \u001b[39m*\u001b[39m K2, np\u001b[39m.\u001b[39mdstack(\n\u001b[0;32m    942\u001b[0m         (K1_gradient \u001b[39m*\u001b[39m K2[:, :, np\u001b[39m.\u001b[39mnewaxis], K2_gradient \u001b[39m*\u001b[39m K1[:, :, np\u001b[39m.\u001b[39mnewaxis])\n\u001b[0;32m    943\u001b[0m     )\n\u001b[0;32m    944\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 945\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mk1(X, Y) \u001b[39m*\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mk2(X, Y)\n",
      "File \u001b[1;32mo:\\Programing\\predict-supply-disruption\\env\\Lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:1535\u001b[0m, in \u001b[0;36mRBF.__call__\u001b[1;34m(self, X, Y, eval_gradient)\u001b[0m\n\u001b[0;32m   1533\u001b[0m length_scale \u001b[39m=\u001b[39m _check_length_scale(X, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlength_scale)\n\u001b[0;32m   1534\u001b[0m \u001b[39mif\u001b[39;00m Y \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 1535\u001b[0m     dists \u001b[39m=\u001b[39m pdist(X \u001b[39m/\u001b[39;49m length_scale, metric\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39msqeuclidean\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m   1536\u001b[0m     K \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mexp(\u001b[39m-\u001b[39m\u001b[39m0.5\u001b[39m \u001b[39m*\u001b[39m dists)\n\u001b[0;32m   1537\u001b[0m     \u001b[39m# convert from upper-triangular matrix to square matrix\u001b[39;00m\n",
      "File \u001b[1;32mo:\\Programing\\predict-supply-disruption\\env\\Lib\\site-packages\\scipy\\spatial\\distance.py:2220\u001b[0m, in \u001b[0;36mpdist\u001b[1;34m(X, metric, out, **kwargs)\u001b[0m\n\u001b[0;32m   2218\u001b[0m     pdist_fn \u001b[39m=\u001b[39m metric_info\u001b[39m.\u001b[39mpdist_func\n\u001b[0;32m   2219\u001b[0m     _extra_windows_error_checks(X, out, (m \u001b[39m*\u001b[39m (m \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m) \u001b[39m/\u001b[39m \u001b[39m2\u001b[39m,), \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m-> 2220\u001b[0m     \u001b[39mreturn\u001b[39;00m pdist_fn(X, out\u001b[39m=\u001b[39;49mout, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   2221\u001b[0m \u001b[39melif\u001b[39;00m mstr\u001b[39m.\u001b[39mstartswith(\u001b[39m\"\u001b[39m\u001b[39mtest_\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m   2222\u001b[0m     metric_info \u001b[39m=\u001b[39m _TEST_METRICS\u001b[39m.\u001b[39mget(mstr, \u001b[39mNone\u001b[39;00m)\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 12.6 GiB for an array with shape (1693940115,) and data type float64"
     ]
    }
   ],
   "source": [
    "pipeline.fit(X_train, y_train)\n",
    "f1_score(y_test, pipeline.predict(X_test), average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importances = pd.DataFrame({'Feature': pipeline['model'].feature_name_, 'Importance': pipeline['model'].feature_importances_})\n",
    "feature_importances.to_csv('features.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_submission(pipeline, 'submission_LGBM_ENS')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.4 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "79663a0a9d8da7c7d65d8c29430beddf43ec80497536ce6768e4360a34723de0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
