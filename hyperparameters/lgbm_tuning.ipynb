{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Подбор гиперпараметров LightGBM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Загрузка модулей"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В качестве библиотеки для подбора гиперпараметров мы будем использовать optuna, модель - LightGBM (для совместимости с пайплайном используется обёртка LGBMClassifier), отбор гиперпараметров будет производиться с помощью двух функций оптимизации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Модель\n",
    "import optuna as opt\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.model_selection import cross_val_score, train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import f1_score, roc_auc_score\n",
    "\n",
    "# Пайплайн\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import TransformerMixin, BaseEstimator\n",
    "\n",
    "# Данные\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from category_encoders import BinaryEncoder\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Предобработчик\n",
    "class DataPreprocessor(BaseEstimator, TransformerMixin):\n",
    "    \"\"\" Предобработчик данных. \n",
    "\n",
    "    Класс предобработчика данных, совмещающий бинарную кодировку \n",
    "    категориальных признаков и масштабирование числовых признаков.\n",
    "    Наследуется от BaseEstimator и TransformerMixin из модуля\n",
    "    base библиотеки scikit-learn для совместимости с пайплайнами\n",
    "    (Pipeline из модуля sklearn.pipeline)\n",
    "    \n",
    "    Параметры:\n",
    "        cat_features: Список категориальных признаков, которые \n",
    "            будут обработаны с помощью бинарного кодировщика.\n",
    "        scale_features: Список числовых признаков, которые \n",
    "            будут обработаны с помощью Robust Scaler.\n",
    "        drop_features: Список признаков, которые будут откинуты и\n",
    "            не участвуют в процессах обучения и предсказания.\n",
    "        rename_cols: Новые именования признаков. Требуется, если\n",
    "            в признаках содержатся символы, не поддерживаемые\n",
    "            моделью. \n",
    "            \n",
    "    Функции:\n",
    "        fit: Обучает предобработчики для дальнейшего использования.\n",
    "        transform: Трансформирует датасет для дальнейшего использования. \"\"\"\n",
    "    \n",
    "    def __init__(self, cat_features: list, scale_features: list,\n",
    "                 drop_features: list, rename_cols: None | list) -> None:\n",
    "        # Инициализация атрибутов\n",
    "        self.cat_features = cat_features\n",
    "        self.rename_cols = rename_cols\n",
    "        self.drop_features = drop_features\n",
    "        self.scale_features = scale_features\n",
    "\n",
    "        # Инициализация предобработчиков\n",
    "        self.bin_encoder = BinaryEncoder(cols=cat_features)\n",
    "        self.robust_scaler = RobustScaler()\n",
    "\n",
    "    def fit(self, X: pd.DataFrame, y= pd.DataFrame | None) -> object:\n",
    "        \"\"\" Обучение предобработчика.\n",
    "\n",
    "        Обучает предобработчики, на основе датасета и дополнительных признаков,\n",
    "        которые выводятся из уже имеющихся (экстракция признаков).\n",
    "        \n",
    "        Параметры: \n",
    "            X: Экземпляр pandas.DataFrame, содержащий независимые переменные.\n",
    "            y: Экземпляр pandas.DataFrame, содержащий зависимые переменные. \n",
    "\n",
    "        Пример:\n",
    "            data_preprocessor = DataPreprocessor(cat_features, scale_features, \n",
    "                                                 drop_features, rename_cols)\n",
    "            data_preprocessor.fit(X_train, y_train)\n",
    "        \n",
    "        Возвращает обученный предобработчик.\"\"\"\n",
    "        \n",
    "        # Создаём копию датасета, чтобы не изменять исходный\n",
    "        X_ = X.copy()\n",
    "        X_.columns = self.rename_cols\n",
    "        \n",
    "        X_['Weekday'] += 1\n",
    "\n",
    "        # Экстракция признаков\n",
    "        X_['Provider Purchaser'] = [f'{x}_{y}' for x, y in zip(X_['Provider'].values, \\\n",
    "                                                               X_['Purchasing Organization'].values)]\n",
    "        X_['Provider Delivery option'] = [f'{x}_{y}' for x, y in zip(X_['Provider'].values, \\\n",
    "                                                                     X_['Delivery Option'].values)]\n",
    "        X_['Sum Fold'] = X_['Sum'].apply(lambda x: int(x) % 10)\n",
    "        X_['ETC Difference'] = X_['Duration'] - X_['ETC Delivery']\n",
    "        X_['Change Difference'] = X_['Delivery Date'] - X_['Change on Paper']\n",
    "        X_['ETC Power'] = X_['ETC Difference'] ^ 2\n",
    "        \n",
    "        # Добавляем тригонометрические значения временных признаков\n",
    "        X_['day_sin'] = np.sin(np.pi * 2 * X_['Weekday'] / 7)\n",
    "        X_['day_cos'] = np.cos(np.pi * 2 * X_['Weekday'] / 7)\n",
    "        X_['month1_sin'] = np.sin(np.pi * 2 * X_['Month1'] / 12)\n",
    "        X_['month1_cos'] = np.cos(np.pi * 2 * X_['Month1'] / 12)\n",
    "        X_['month2_sin'] = np.sin(np.pi * 2 * X_['Month2'] / 12)\n",
    "        X_['month2_cos'] = np.cos(np.pi * 2 * X_['Month2'] / 12)\n",
    "        X_['month3_sin'] = np.sin(np.pi * 2 * X_['Month3'] / 12)\n",
    "        X_['month3_cos'] = np.cos(np.pi * 2 * X_['Month3'] / 12)\n",
    "\n",
    "        # Нормализация числовых признаков\n",
    "        self.robust_scaler.fit(X_[self.scale_features])\n",
    "\n",
    "        # Кодировка категориальных признаков\n",
    "        X_ = self.bin_encoder.fit_transform(X_)\n",
    "\n",
    "        # Дроп неиспользуемых признаков\n",
    "        X_ = X_.drop(self.drop_features, axis=1)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def transform(self, X) -> pd.DataFrame:\n",
    "        \"\"\" Трансформирование датасета.\n",
    "        \n",
    "        Трансформирует датасет для дальнешего использования моделью.\n",
    "        Требует предварительного обучения с помощью метода fit.\n",
    "        \n",
    "        Параметры: \n",
    "            X: Экземпляр pandas.DataFrame, содержащий независимые переменные. \n",
    "            \n",
    "        Возвращает трансформированный датасет (экземпляр pandas.DataFrame), \n",
    "        готовый для использования моделью.\n",
    "        \n",
    "        Пример:\n",
    "            data_preprocessor = DataPreprocessor(cat_features, scale_features, \n",
    "                                                 drop_features, rename_cols)\n",
    "            data_preprocessor.fit(X_train, y_train)\n",
    "            X_preprocessed = data_preprocessor.transform(X_test)\"\"\"\n",
    "\n",
    "        # Создаём копию датасета, чтобы не изменять исходный\n",
    "        X_ = X.copy()\n",
    "        X_.columns = self.rename_cols\n",
    "\n",
    "        X_['Weekday'] += 1\n",
    "\n",
    "        # Экстракция фич\n",
    "        X_['Provider Purchaser'] = [f'{x}_{y}' for x, y in zip(X_['Provider'].values, X_['Purchasing Organization'].values)]\n",
    "        X_['Provider Delivery option'] = [f'{x}_{y}' for x, y in zip(X_['Provider'].values, X_['Delivery Option'].values)]\n",
    "        X_['Sum Fold'] = X_['Sum'].apply(lambda x: int(x) % 10)\n",
    "        X_['ETC Difference'] = X_['Duration'] - X_['ETC Delivery']\n",
    "        X_['Change Difference'] = X_['Delivery Date'] - X_['Change on Paper']\n",
    "        X_['ETC Power'] = X_['ETC Difference'] ^ 2\n",
    "\n",
    "        # Временные фичи\n",
    "        X_['day_sin'] = np.sin(np.pi * 2 * X_['Weekday'] / 7)\n",
    "        X_['day_cos'] = np.cos(np.pi * 2 * X_['Weekday'] / 7)\n",
    "        X_['month1_sin'] = np.sin(np.pi * 2 * X_['Month1'] / 12)\n",
    "        X_['month1_cos'] = np.cos(np.pi * 2 * X_['Month1'] / 12)\n",
    "        X_['month2_sin'] = np.sin(np.pi * 2 * X_['Month2'] / 12)\n",
    "        X_['month2_cos'] = np.cos(np.pi * 2 * X_['Month2'] / 12)\n",
    "        X_['month3_sin'] = np.sin(np.pi * 2 * X_['Month3'] / 12)\n",
    "        X_['month3_cos'] = np.cos(np.pi * 2 * X_['Month3'] / 12)\n",
    "\n",
    "        # Нормализация\n",
    "        X_[self.scale_features] = self.robust_scaler.transform(X_[self.scale_features])\n",
    "\n",
    "        # Категориальные фичи\n",
    "        X_ = self.bin_encoder.transform(X_)\n",
    "\n",
    "        X_ = X_.drop(self.drop_features, axis=1)\n",
    "\n",
    "        return X_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_f1_macro(trial: opt.Trial) -> float:\n",
    "    \"\"\" Функция оптимизации F1 (macro).\n",
    "    \n",
    "    Использует библиотеку optuna для подбора параметров из\n",
    "    заданного диапазона. В качестве оптимизируемой метрики \n",
    "    выступает F1 (macro).\n",
    "    \n",
    "    Параметры:\n",
    "        trial: экзмепляр optuna.Trial, представляющей собой\n",
    "        историю оптимизации целевой функции.\n",
    "        \n",
    "    Пример:\n",
    "        study = optuna.create_study(direction='maximize')\n",
    "        study.optimize(objective, n_trials=100)\n",
    "        \n",
    "    Возвращает F1 (macro) метрику для подобранных параметров.\"\"\"\n",
    "    \n",
    "    # Параметры\n",
    "    learning_rate = trial.suggest_float('learning_rate', 0.01, 1)\n",
    "    n_estimators = trial.suggest_int('n_estimators', 1300, 3000)\n",
    "    max_depth = trial.suggest_int('max_depth', 6, 21)\n",
    "    max_bin = trial.suggest_int('max_bin', 64, 200),\n",
    "    num_leaves = trial.suggest_int('num_leaves', 32, 260)\n",
    "    reg_lambda = trial.suggest_float('l2_reg', 0.01, 1)\n",
    "\n",
    "    # Модель\n",
    "    data_preprocessor = DataPreprocessor(cat_features, scale_features, \n",
    "                                         drop_features, rename_cols)\n",
    "    model = LGBMClassifier(\n",
    "        learning_rate=learning_rate,\n",
    "        n_estimators=n_estimators,\n",
    "        max_depth=max_depth,\n",
    "        num_leaves=num_leaves,\n",
    "        reg_lambda=reg_lambda,\n",
    "        max_bin=max_bin,\n",
    "        force_col_wise=True\n",
    "    )\n",
    "\n",
    "    pipeline = Pipeline([\n",
    "        ('data_preproc', data_preprocessor),\n",
    "        ('model', model)\n",
    "    ])\n",
    "    \n",
    "    cv_score = cross_val_score(pipeline, X_general, y_general, cv=StratifiedKFold(n_splits=5), scoring='f1_macro', n_jobs=-1)\n",
    "    accuracy = cv_score.mean()\n",
    "\n",
    "    return accuracy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
